{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_Cleansing\n",
    "#### Author: Armin Berger\n",
    "\n",
    "Date: 03/10/2020\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 2.7.11 and Jupyter notebook\n",
    "\n",
    "Libraries used: \n",
    "* re \n",
    "* os \n",
    "* pandas \n",
    "* numpy \n",
    "* nltk.sentiment.vader SentimentIntensityAnalyzer\n",
    "* haversine, Unit\n",
    "* math\n",
    "* sklearn.linear_model LinearRegression\n",
    "* matplotlib\n",
    "\n",
    "\n",
    "### Overview:\n",
    "\n",
    "This project contains three main parts.\n",
    "\n",
    "- Wrangling dirty_data\n",
    "- Removing outliers\n",
    "- Imputing missing values \n",
    "\n",
    "These three core tasks are underlined in red to help the reader. Moreover, each task is devided into enumerated subtasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries \n",
    "\n",
    "Firstly, we will import all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the required libraries for this assignment \n",
    "\n",
    "import pandas as pd # used for dataframe maniulation and general wrangling\n",
    "\n",
    "import numpy as np # used for mathematical operations\n",
    "\n",
    "import re # used for regular expression\n",
    "\n",
    "import os # used for reading in multiple file from the local os \n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer # used to get sentiment of text\n",
    "\n",
    "import math # used for linear operations\n",
    "\n",
    "from sklearn.linear_model import LinearRegression # used for linear regression models\n",
    "\n",
    "from matplotlib import pyplot as plt # used for plotting histograms and boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> 1. Wrangling dirty_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of this project we read in the csv file '26255367_dirty_data.csv' and remove both syntactical and semantic anomalies/erros.\n",
    "Firtly, we will detect and remove all syntactical anomalies. Following that, all semantic erros will be removed. For both steps we will start with a 'dirty dataframe' and then over the course of identifying and fixing errors in rows we will add these rows to a 'clean dataframe'. This is possible since it was stated in the assignment brief, that each row can only containe a maximum of one syntactic or semantic error. Thus, if an error is fixed in a certain row that row can't have any further anomalies and can be added to the 'clean dataframe'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\"> A. Identify and Remove Syntactical Anomalies \n",
    "    \n",
    "In this step we will fix all errors that are realted to format issue (syntactic). We will go through the dataframe column by column, starting with the order_id. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1 Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in our dirty data in csv format\n",
    "dirty_data_df = pd.read_csv('26255367_dirty_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display first 5 rows to get an idea of the structure and type of data\n",
    "dirty_data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many rows and columns the df has\n",
    "dirty_data_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows information of all columns\n",
    "dirty_data_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the info() command we can see that there are no null values in the entire data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2 Check if order_ids are all unique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# since each order is unique, there should be no duplicate order_ids \n",
    "# we use the describe function to check for duplciates\n",
    "# only select columns with data type object (string like data type)\n",
    "dirty_data_df.describe(include=['O'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using the command .describe(include=['O']) we could see that there we 500 unique order_ids, thus no duplicates need to be removed. Futhermore, it could be obserevd that the dataset contains some duplicate customer_ids. This is unproblematic, however, since the same customer could palce multiple unique order, thus no actions are required for the customer_id as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3 Check if the dates are in correct format (YYY-MM-DD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we seek to detect and isolate all date values that don't match our pattern of YYYY-MM-DD\n",
    "# done with the to_datetime function\n",
    "\n",
    "# scan through the date column of all rows\n",
    "# in case the date format doesn't fit our desired format we set the row in date_mask = True \n",
    "date_mask = pd.to_datetime(dirty_data_df.date, format='%Y-%m-%d', errors='coerce').isna()\n",
    "\n",
    "# now we extract all the rows which have an index matching the index value saved in date_mask\n",
    "# the loc[] function helps us filter out these rows and save them in dirty_date\n",
    "dirty_date = dirty_data_df.loc[date_mask, :]\n",
    "\n",
    "# check how many rows have a faulty date\n",
    "dirty_date.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this data is all the columns that have faulty date formats\n",
    "# this 'visual analysis' enables us to check for the kind of date format issues we need to fix\n",
    "dirty_date.head(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after selecting all rows with a faulty date entry we need to correct the date format\n",
    "# for that purpose we have created a function\n",
    "\n",
    "def date_change(wrong_date):\n",
    "    \n",
    "    # we split the date on -\n",
    "    pieces_date = wrong_date.split('-')\n",
    "    \n",
    "    # set the year  \n",
    "    year = (pieces_date[0])\n",
    "    \n",
    "    # set the month  \n",
    "    month = (int)(pieces_date[1])\n",
    "    \n",
    "    # set the days of each month\n",
    "    day = (int)(pieces_date[2]) \n",
    "    \n",
    "    # check if month and year were switched\n",
    "    # if yes we swap them\n",
    "    if len(year) != 4:\n",
    "        year, day = day,(int)(year)\n",
    "    \n",
    "    # check if the month is larger than 12\n",
    "    # if yes than switch month for days\n",
    "    if month > 12:\n",
    "        day, month = month,day\n",
    "    \n",
    "    # check if months with 30 days have more than 30 days\n",
    "    # if yes than set them to 30 days \n",
    "    if month in (4,6,9,11):\n",
    "        if day > 30:\n",
    "            day = 30\n",
    "    \n",
    "    # check the month of february\n",
    "    # set days to 28 incase the given number is large than that\n",
    "    if month == 2:\n",
    "        if day > 28:\n",
    "            day = 28\n",
    "    \n",
    "    \n",
    "    # now reconstruct the deconstructed date \n",
    "    # we combine the year, month, and day as strings and save them as a proper date fromat\n",
    "    formatted_date = pd.to_datetime(str(year)+\"-\"+str(month)+\"-\"+str(day),format='%Y-%m-%d')\n",
    "    \n",
    "    # return the edited dates\n",
    "    return formatted_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the function date_change to edit all the selected dates in faulty format\n",
    "dirty_data_df['date'] = dirty_data_df['date'].apply(date_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-using the same code form above we check if there are any dates left in faulty format\n",
    "date_mask = pd.to_datetime(dirty_data_df.date, format='%Y-%m-%d', errors='coerce').isna()\n",
    "dirty_date = dirty_data_df.loc[date_mask, :]\n",
    "\n",
    "# check how many rows still have faulty dates left\n",
    "dirty_date.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.4 Check if the nearest_warehouse values are spelled correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the names of the three warehouse \n",
    "warehouse_df = pd.read_csv('warehouses.csv')\n",
    "\n",
    "# display the names of the three warehouses\n",
    "warehouse_df['names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the spelling of the nearest_warehouse names \n",
    "warehouse_count = dirty_data_df['nearest_warehouse'].value_counts()\n",
    "\n",
    "# display the value counts\n",
    "warehouse_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the same warehouse is not always spelt consistently, thus we need to set them to a consistent spelling.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a function to edit and uniform all the nearest_warehouse names \n",
    "# we fix nearest_warehouse names based on the starting letter of nearest_warehouse\n",
    "def fix_nearest_warehouse(warehouse):\n",
    "    \n",
    "    # if a word starts with the letter b\n",
    "    if warehouse.startswith('b') or warehouse.startswith('B') :\n",
    "        \n",
    "        # we return the correctly spelled name of warehouse 'Bakers'\n",
    "        return 'Bakers'\n",
    "    \n",
    "    # if a word starts with the letter n\n",
    "    elif warehouse.startswith('n') or warehouse.startswith('N'):\n",
    "        \n",
    "        # we return the correctly spelled name of warehouse 'Nickolson'\n",
    "        return 'Nickolson'\n",
    "    \n",
    "    # if a word starts with the letter t\n",
    "    elif warehouse.startswith('t') or warehouse.startswith('T'):\n",
    "        \n",
    "        # we return the correctly spelled name of warehouse 'Thompson'\n",
    "        return 'Thompson'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we go through all the values for nearest_warehouse and fix their spelling\n",
    "dirty_data_df['nearest_warehouse'] = dirty_data_df.apply(lambda row:fix_nearest_warehouse(row['nearest_warehouse']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final check if transformation worked and warehouses have uniform names \n",
    "warehouse_count = dirty_data_df['nearest_warehouse'].value_counts()\n",
    "warehouse_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the warhouses have uniform names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.5 Check if items in shopping_cart are uniformly spelled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we seek to check if all the individual items in the shopping_cart column are spelled uniformly\n",
    "# catch all the individual items using regular expression and save them in a list\n",
    "lst = list(dirty_data_df['shopping_cart'].apply(lambda x: re.findall(\"\\(\\'(.+?)\\', \",x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have all the individual items in a list of lists we seek to merge them and only keep the unique ones\n",
    "# using list comprehension unpack the list of lists and save the output as a set \n",
    "unique_items = set([item for lists in lst for item in lists])\n",
    "\n",
    "# display the unique items\n",
    "unique_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output shows that there are 10 unique items in the column shopping_cart, thus there are no duplicates or missspellings to fix up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.6 Checking customer_lat and customer_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the longitude values using the describe() methode\n",
    "long_count = dirty_data_df['customer_long'].describe()\n",
    "\n",
    "# display the statistics\n",
    "long_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the longitude values\n",
    "dirty_data_df.hist(column='customer_long')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that there are incorrect longitude values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the latitude values using the describe() methode\n",
    "lat_count = dirty_data_df['customer_lat'].describe()\n",
    "\n",
    "# display the statistics\n",
    "lat_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the latitude values\n",
    "dirty_data_df.hist(column='customer_lat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that there are incorrect longitude values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melbourne has a latitude of -37.840935 and a longitude of 144.946457\n",
    "# when skimming over the data we saw customer_lat values starting with 144 and customer_long values starting with\n",
    "# -37, which clearly indicates that they are interchanged\n",
    "# therefore we need to find a way to swap them \n",
    "\n",
    "# we identify all the rows in which the longitude and latitude were mixed up and save them in a sub data frame\n",
    "interchanged_df = dirty_data_df[(dirty_data_df['customer_lat'] > 0) & (dirty_data_df['customer_long'] < 0)]\n",
    "\n",
    "# create a series in which we can save whether in a row the longitude and latitude was mixed up\n",
    "interchange_idx = (interchanged_df['customer_lat'] > 0) & (interchanged_df['customer_long'] < 0)\n",
    "\n",
    "# check the rows where longitude and latitude need to be fixed\n",
    "interchanged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the rows in which the lat and long of customer's are interchanged, are now being swapped \n",
    "# the function .loc only takes the rows where interchange_idx matches \n",
    "interchanged_df.loc[interchange_idx,['customer_lat','customer_long']] = interchanged_df.loc[interchange_idx,\\\n",
    "                                                                       ['customer_long','customer_lat']].values\n",
    "\n",
    "# display the top 10 rows of the interchanged_df to see if the swapping worked\n",
    "interchanged_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after swapping longitude and latitude in all the rows where they were mixed up we now check\n",
    "# if longitude and latitude values have undesired + or - signs \n",
    "\n",
    "# ~ is used for filtering, it only keep the rows where the latitude and lonitude were not changed up\n",
    "dirty_data_df_new = dirty_data_df[~dirty_data_df['order_id'].isin(interchanged_df['order_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that checks whether lat is larger than 0\n",
    "# required to get correct latitude values \n",
    "def check_lat(lat):\n",
    "    \n",
    "    if lat > 0:\n",
    "        \n",
    "        # since australia is in the southern hemisphere the latitude values need to\n",
    "        # be negative\n",
    "        lat = -lat\n",
    "    \n",
    "    # return correct lat\n",
    "    return lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we apply the check_lat() function to the remanining part of the dataframe \n",
    "dirty_data_df_new.loc[:,'customer_lat'] = dirty_data_df_new.apply(lambda row:check_lat(row['customer_lat']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that checks whether lon is smaller than 0\n",
    "# required to get correct longitude values \n",
    "def check_lon(lon):\n",
    "    \n",
    "    if lon < 0:\n",
    "        \n",
    "        # since australia only has positive longitude values, any negative value needs to\n",
    "        # be turned positive using abs()\n",
    "        lon = abs(lon)\n",
    "    \n",
    "    # return correct lon\n",
    "    return lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we apply the check_lon() function to the remanining part of the dataframe \n",
    "dirty_data_df_new.loc[:,'customer_long'] = dirty_data_df_new.apply(lambda row:check_lon(row['customer_long']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the end we combine the two dataframes from the \n",
    "dirty_data_df = pd.concat([interchanged_df , dirty_data_df_new])\n",
    "\n",
    "# check if we have all columns in one dataframe again\n",
    "dirty_data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do a last check if we were able to successfully fix up the customer_lat and customer_long columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display statistics of 'customer_lat'\n",
    "dirty_data_df['customer_lat'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display statistics of 'customer_long'\n",
    "dirty_data_df['customer_long'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. 7 Checking the spelling of season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the spelling of the seasons\n",
    "season_count = dirty_data_df['season'].value_counts()\n",
    "\n",
    "# display the count of seaon values\n",
    "season_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this shows we need to standardise season names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a function to edit and standardise all the season names\n",
    "# we fix season names based on the starting letter of the seaosn\n",
    "def fix_season(season_input):\n",
    "    \n",
    "    # set season input string to lower case\n",
    "    season_input = season_input.lower()\n",
    "    \n",
    "    # in case string input starts with 'au.' return 'Autumn'\n",
    "    if re.search('au.',season_input) :\n",
    "        \n",
    "        # return correct season name\n",
    "        return 'Autumn'\n",
    "    \n",
    "    # in case string input starts with 'su.' return 'Summer'\n",
    "    elif re.search('su.',season_input) :\n",
    "        \n",
    "        # return correct season name\n",
    "        return 'Summer'\n",
    "    \n",
    "    # in case string input starts with 'sp.' return 'Spring'\n",
    "    elif re.search('sp.',season_input) :\n",
    "        \n",
    "        # return correct season name\n",
    "        return 'Spring'\n",
    "    \n",
    "    # in case string input starts with 'wi.' return 'Winter'\n",
    "    elif re.search('wi.',season_input) :\n",
    "        \n",
    "        # return correct season name\n",
    "        return 'Winter'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we go through all the cloumn values for season and fix their spelling\n",
    "dirty_data_df['season'] = dirty_data_df.apply(lambda row:fix_season(row['season']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the spelling of the nearest_warehouse names \n",
    "season_count = dirty_data_df['season'].value_counts()\n",
    "\n",
    "# display the season values to afirm their uniformity\n",
    "season_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. 8 Checking expedited delivery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if expedited delivery has correct boolean variables\n",
    "expedited_count = dirty_data_df['is_expedited_delivery'].value_counts()\n",
    "\n",
    "# display values\n",
    "expedited_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column is_expedited_delivery only has two boolean values (True/False), thus no changes need to be made. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.9 Checking is_happy_customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if is_happy_customer has correct boolean variables\n",
    "is_happy_customer_count = dirty_data_df['is_happy_customer'].value_counts()\n",
    "\n",
    "# display values\n",
    "is_happy_customer_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column is_happy_customer only has two boolean values (True/False), thus no changes need to be made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\"> B. Identify and Remove Semantic Anomalies "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After identifying all syntactic errors we will now focus on fixing up all semantic errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to save the index value of all rows that were already fixed up\n",
    "# this is useful since all rows can only have one semantic error\n",
    "fixed_columns = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.1 Check if season value matches with date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check is all orders follow correct order time, that don't match the specified condition\n",
    "# to check whether the season matches with the actual date we created we created \n",
    "# the function fix_season() \n",
    "# assigns a season given the date\n",
    "def check_season(date, season):\n",
    "    \n",
    "    # turn input into string type\n",
    "    date = str(date)\n",
    "    \n",
    "    # we split the date on -\n",
    "    pieces_date = date.split('-')\n",
    "    \n",
    "    # save the month value as an int\n",
    "    month = (int)( pieces_date[1])\n",
    "    \n",
    "    # spring is month 09-11\n",
    "    if month > 8 and month < 12 and season == 'Spring':\n",
    "        return True\n",
    "        \n",
    "    # summer is month 12-02\n",
    "    elif month == 12 or month == 1 or month == 2 and season == 'Summer':\n",
    "        return True\n",
    "    \n",
    "    # autumn is month 3-5\n",
    "    elif month > 2 and month < 6 and season == 'Autumn':\n",
    "        return True\n",
    "    \n",
    "    # winter is month 6-8\n",
    "    elif month > 5 and month < 9and season == 'Winter':\n",
    "        return True\n",
    "    \n",
    "    # if any of the above defined cases don't match return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that assigns a season given the date provided for each row\n",
    "# fixes faulty dates\n",
    "def fix_season(date):\n",
    "    \n",
    "    # turn input into string type\n",
    "    date = str(date)\n",
    "    \n",
    "    # we split the date on -\n",
    "    pieces_date = date.split('-')\n",
    "    \n",
    "    # save the month value as an int\n",
    "    month = (int)( pieces_date[1])\n",
    "    \n",
    "    # spring is month 09-11\n",
    "    if month > 8 and month < 12:\n",
    "        season = 'Spring'\n",
    "        \n",
    "    # summer is month 12-02\n",
    "    elif month == 12 or month == 1 or month == 2:\n",
    "        season = 'Summer'\n",
    "    \n",
    "    # autumn is month 3-5\n",
    "    elif month > 2 and month < 6:\n",
    "        season = 'Autumn'\n",
    "    \n",
    "    # winter is month 6-8\n",
    "    elif month > 5 and month < 9:\n",
    "        season = 'Winter'\n",
    "    \n",
    "    # return correct season value\n",
    "    return season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all the rows with faulty season values given the date provided\n",
    "season_false = dirty_data_df.apply(lambda row: check_season(row['date'], row['season']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the index value of all the rows with faulty season values given the date provided to the list\n",
    "# fixed_columns\n",
    "[fixed_columns.append(x) for x in list(dirty_data_df.index[season_false == False])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function to all rows for the column oder_type\n",
    "# since we need two columns both the order type and time we use a lambda within the apply function\n",
    "dirty_data_df['season'] = dirty_data_df.apply(lambda row:fix_season(row['date']),axis=1)\n",
    "\n",
    "# manuly checking if our manipulation was succesfull\n",
    "dirty_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2 Check if customer was satisfied with most recent order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to assess whether a customer was satisfied with most recent order or not we will use the SentimentIntensityAnalyzer() that is part of nltk. Using SentimentIntensityAnalyzer() we generate a polarity_scores that is saved in a seperate column of our main dataframe dirty_data_df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the spelling of the nearest_warehouse names \n",
    "is_happy_customer_count = dirty_data_df['is_happy_customer'].value_counts()\n",
    "\n",
    "# display values \n",
    "is_happy_customer_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that creates polarity_scores\n",
    "\n",
    "def get_sentiment(input):\n",
    "    \n",
    "    # save the SentimentIntensityAnalyzer()\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # calculate polarity score\n",
    "    sentiment = analyzer.polarity_scores(input)\n",
    "    \n",
    "    # return calculated polarity score\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check whether the is_happy_customer boolean values matches the sentiment scores calculated \n",
    "def check_happy(score,is_happy):\n",
    "    \n",
    "    # scores larger or equal to 0.05 are indicative of a postive sentiment\n",
    "    # thus the is_happy variable should be positive\n",
    "    if score  >= 0.05 and is_happy == True: \n",
    "        \n",
    "        # if score = postive and is_happy = true we return a True value\n",
    "        return True\n",
    "\n",
    "    # scores smaller than 0.05 are indicative of a negative sentiment\n",
    "    # thus the is_happy variable should be negative\n",
    "    elif score < 0.05 and is_happy == False:\n",
    "        \n",
    "        # if score = negative and is_happy = false we return a True value\n",
    "        return True\n",
    "    \n",
    "    # in all other cases we return a False value\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fix the is_happy_customer boolean values \n",
    "def fix_happy(score):\n",
    "    \n",
    "    # return happy True if sentiment_score >= 0.05\n",
    "    if score  >= 0.05: \n",
    "        return True\n",
    "    \n",
    "    # return happy False in all other case\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function get_sentiment to the column latest_customer_review to calculate a sentiment score\n",
    "dirty_data_df['sentiment_score'] = dirty_data_df.apply(lambda row: get_sentiment(row['latest_customer_review'])[\"compound\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the sentiment_scores we just calculated\n",
    "# as the histogram shows most of the reviews have posituive sentiment scores\n",
    "dirty_data_df.hist(column='sentiment_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seletc and save the rows in which the sentiment score and is_happy_customer didn't match \n",
    "correct_happy_bool = dirty_data_df.apply(lambda row: check_happy(row['sentiment_score'],row['is_happy_customer']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the index values of these selected rows to the list fixed_columns\n",
    "[fixed_columns.append(x) for x in list(dirty_data_df.index[correct_happy_bool == False])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after identifying all the misslabelings in the is_happy_customer column\n",
    "# we fix them by applying the fix_happy methode to dirty_data_df['is_happy_customer'] column\n",
    "dirty_data_df['is_happy_customer'] = dirty_data_df.apply(lambda row: fix_happy(row['sentiment_score']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lastly check if we succesfully removed all misslabelings in the is_happy_customer column\n",
    "correct_happy_bool = dirty_data_df.apply(lambda row: check_happy(row['sentiment_score'],row['is_happy_customer']),axis=1)\n",
    "\n",
    "# display the values\n",
    "correct_happy_bool.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The is_happy_customer column is successfully fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.3 Check if nearest_warehouse values are really the nearest warehouse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be assessed whether the values in column nearest_warehouse are actually the the nearest warehouses to each\n",
    "customer. This is done by using the read in coordinate information of of the warehouses and compare them with the customer's coordinates. The we will calculate the distance to each warehouse and choose the shortest distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the names and locaction of the three warehouse \n",
    "warehouse_df = pd.read_csv('warehouses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check lat and lon values of the three warehouses\n",
    "warehouse_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data_df['nearest_warehouse'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate distance to nearest warehouse\n",
    "def cal_distance(warehouse, customer):\n",
    "    \n",
    "    # save the coordinates required to calculate the distance \n",
    "    lat_1, long_1 = warehouse\n",
    "    lat_2, long_2 = customer\n",
    "\n",
    "    # save the radius of the earth in m\n",
    "    radius = 6378000  \n",
    "    phi_1 = math.radians(lat_1)\n",
    "    phi_2 = math.radians(lat_2)\n",
    "\n",
    "    # haversine calculations required to determine distance to nearest warehouse\n",
    "    delta_phi = math.radians(lat_2 - lat_1)\n",
    "    delta_lambda = math.radians(long_2 - long_1)\n",
    "\n",
    "    a = math.sin(delta_phi / 2.0) ** 2 + math.cos(phi_1) * math.cos(phi_2) * math.sin(delta_lambda / 2.0) ** 2\n",
    "    \n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    \n",
    "    # the distance in meters \n",
    "    meters = radius * c \n",
    "    \n",
    "    # the distance in kilometers\n",
    "    km = meters / 1000.0\n",
    "    \n",
    "    # rounding the meter and km output\n",
    "    meters = round(meters, 4)\n",
    "    km = round(km, 4)\n",
    "    \n",
    "    # return the final rounded distance to warehouse in km\n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create methode that calaculates the distance to the nearest warehosue and retuns the \n",
    "# name of that warehouse \n",
    "def compare_dist_warehouse(cust_lat, cust_lon):\n",
    "    \n",
    "    # save customer postion as  tuple\n",
    "    cust_cor = (cust_lat, cust_lon)\n",
    "    \n",
    "    # list of distances to warehouse\n",
    "    dist_warehouse_list = []\n",
    "    \n",
    "    ## First, get all warehouse locations\n",
    "    \n",
    "    # save location of 'Nickolson' warehouse as tuple (lon,lat)\n",
    "    warehouse_loc_nick = (warehouse_df.loc[0,'lat'], warehouse_df.loc[0,'lon'])\n",
    "    \n",
    "    # save location of 'Thompson' warehouse as tuple (lon,lat)\n",
    "    warehouse_loc_thomp = (warehouse_df.loc[1,'lat'], warehouse_df.loc[1,'lon'])\n",
    "    \n",
    "    # save location of 'Bakers' warehouse as tuple (lon,lat)\n",
    "    warehouse_loc_baker = (warehouse_df.loc[2,'lat'], warehouse_df.loc[2,'lon'])\n",
    "    \n",
    "    \n",
    "    ## Calculate the distance from the customer to each warehouse\n",
    "    \n",
    "    # distance from customer to warehouse 'Nickolson'\n",
    "    dist_to_nick =  float(cal_distance(warehouse_loc_nick, cust_cor))\n",
    "    \n",
    "    # append calculated distance to list \n",
    "    dist_warehouse_list.append((dist_to_nick, 'Nickolson'))\n",
    "    \n",
    "    # distance from customer to warehouse 'Thompson'\n",
    "    dist_to_thomp =   float(cal_distance(warehouse_loc_thomp, cust_cor))\n",
    "    \n",
    "    # append calculated distance to list \n",
    "    dist_warehouse_list.append((dist_to_thomp, 'Thompson'))\n",
    "    \n",
    "    # distance from customer to warehouse 'Bakers'\n",
    "    dist_to_baker =   float(cal_distance(warehouse_loc_baker, cust_cor))\n",
    "    \n",
    "    # append calculated distance to list \n",
    "    dist_warehouse_list.append((dist_to_baker, 'Bakers'))\n",
    "    \n",
    "    # reverese sort the tuples in the list base on the frist value (distance)\n",
    "    # in each value \n",
    "    dist_warehouse_list.sort(key = lambda x: x[0])\n",
    "    \n",
    "    # return the first item of the list, which is the warehouse with the shortest distance \n",
    "    \n",
    "    return dist_warehouse_list[0][1]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the warhouse which is the nearest warehouse to the customer\n",
    "dirty_data_df['nearest_warehouse'] = dirty_data_df.apply(lambda row: compare_dist_warehouse(row['customer_lat'],row['customer_long']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data_df['nearest_warehouse'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.4 Check if distance_to_nearest_warehouse is correct given the provided warehouse value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be assessed whether the column distance_to_nearest_warehouse matches the actual distance to the declared warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess whether column distance_to_nearest_warehouse has any errors and fix them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation haversine procedure was adapted from \n",
    "# https://community.esri.com/groups/coordinate-reference-systems/blog/2017/10/05/haversine-formula\n",
    "\n",
    "# function to calculate distance to nearest warehouse\n",
    "def cal_warehouse_dist(warehouse, lat, lon):\n",
    "    \n",
    "    # check which warehouse is nearest to customer and save the coordinates \n",
    "    if warehouse == 'Nickolson':\n",
    "        warehouse_loc = (warehouse_df.loc[0,'lon'],warehouse_df.loc[0,'lat'])\n",
    "    \n",
    "    elif warehouse == 'Thompson':\n",
    "        warehouse_loc = (warehouse_df.loc[1,'lon'],warehouse_df.loc[1,'lat'])\n",
    "        \n",
    "    elif warehouse == 'Bakers':\n",
    "        warehouse_loc = (warehouse_df.loc[2,'lon'],warehouse_df.loc[2,'lat'])\n",
    "\n",
    "    # save the coordinates required to calculate the distance \n",
    "    long_1, lat_1 = warehouse_loc\n",
    "    long_2, lat_2 = (lon, lat)\n",
    "\n",
    "    # save the radius of the earth in m\n",
    "    radius = 6378000  \n",
    "    phi_1 = math.radians(lat_1)\n",
    "    phi_2 = math.radians(lat_2)\n",
    "\n",
    "    # haversine calculations required to determine distance to nearest warehouse\n",
    "    delta_phi = math.radians(lat_2 - lat_1)\n",
    "    delta_lambda = math.radians(long_2 - long_1)\n",
    "\n",
    "    a = math.sin(delta_phi / 2.0) ** 2 + math.cos(phi_1) * math.cos(phi_2) * math.sin(delta_lambda / 2.0) ** 2\n",
    "    \n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    \n",
    "    # the distance in meters \n",
    "    meters = radius * c \n",
    "    \n",
    "    # the distance in kilometers\n",
    "    km = meters / 1000.0\n",
    "    \n",
    "    # rounding the meter and km output\n",
    "    meters = round(meters, 4)\n",
    "    km = round(km, 4)\n",
    "    \n",
    "    # return the final rounded distance to warehouse in km\n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that checks whether the given distance and calculated distance match\n",
    "# if yes return True if not then return False  \n",
    "def check_if_warehouse_dist_cor(given_distance,correct_distance):\n",
    "    \n",
    "    # check for the difference between the given_distance and the correct_distance\n",
    "    # we used <= 0.01 to provide a small margin of error\n",
    "    if float(given_distance) - float(correct_distance) <= 0.01:\n",
    "        \n",
    "        # if distance is almost, is the same return True\n",
    "        return True\n",
    "    \n",
    "    # in all other cases we return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that checks whether the given distance and calculated distance match\n",
    "# if not then replace given distance with calculated distance \n",
    "def fix_dist_to_nearest_warehouse(given_distance,correct_distance):\n",
    "    \n",
    "    # ensure that input is in float format\n",
    "    given_distance = float(given_distance)\n",
    "    correct_distance = float(correct_distance)\n",
    "    \n",
    "    # check is given_distance is unequal correct_distance\n",
    "    # if not return correct_distance\n",
    "    if given_distance - correct_distance > 0.01:\n",
    "        \n",
    "        return correct_distance\n",
    "    \n",
    "    # if almost equal to correct_distance return given_distance\n",
    "    else:\n",
    "        \n",
    "        return given_distance\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the correct distance from a customer to the nearest warehouse \n",
    "dirty_data_df['cor_distance_to_nearest_warehouse'] = dirty_data_df.apply(lambda row: cal_warehouse_dist(row['nearest_warehouse'],row['customer_lat'],row['customer_long']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the provided distance from a customer to the nearest warehouse is correct\n",
    "# for all incorrect distances, row index is saved \n",
    "warehouse_dist_false_true = dirty_data_df.apply(lambda row: check_if_warehouse_dist_cor(row['distance_to_nearest_warehouse'],row['cor_distance_to_nearest_warehouse']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the index values of these incorrect distances to the list fixed_columns list\n",
    "[fixed_columns.append(x) for x in list(dirty_data_df.index[warehouse_dist_false_true == False])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we fix all incorrect distances by replacing the incorrect given distance by replacing it with\n",
    "# our calculateddistance\n",
    "dirty_data_df['distance_to_nearest_warehouse'] = dirty_data_df.apply(lambda row: fix_dist_to_nearest_warehouse(row['distance_to_nearest_warehouse'],row['cor_distance_to_nearest_warehouse']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final check if given distance is equal to correect distance \n",
    "dirty_data_df.apply(lambda row: check_if_warehouse_dist_cor(row['distance_to_nearest_warehouse'],row['cor_distance_to_nearest_warehouse']),axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance_to_nearest_warehouse values are all correct now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.5 Calculate the price of each shopping cart item and check the order price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check whether the order price is correct we need to determine the price of individual items. That can be done using a set of multilinear equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to solve a multilinear equation we need reliable data\n",
    "# thus we need to use rows in which we already determined and\n",
    "# fixed semantic erros, the index of these rows are saved in fixed_columns\n",
    "correct_rows = dirty_data_df.loc[fixed_columns]\n",
    "\n",
    "# only keep the shopping cart and order price\n",
    "correct_rows = correct_rows[['shopping_cart','order_price']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that only keeps the data needed of each shopping cart row entry\n",
    "# which is product name and quantity\n",
    "def clean_input(input):\n",
    "    \n",
    "    # save input in string format\n",
    "    input = str(input)\n",
    "    \n",
    "    # list to save ordered items and their quantity in\n",
    "    order_list = list()\n",
    "    \n",
    "    # split the input base on '),' since each shopping cart item is sperated by () and a ,\n",
    "    input = input.split('),')\n",
    "    \n",
    "    # iterat through the list of items\n",
    "    for i in input:\n",
    "        \n",
    "        # find the name of the item\n",
    "        item = re.findall(r'\\'(.+)\\',', i)\n",
    "        \n",
    "        # find it's quantity\n",
    "        quantity = re.findall(r'\\'.+\\',.([0-9])', i)\n",
    "        \n",
    "        # save the item and it's quantity\n",
    "        part = (item[0],int(quantity[0]))\n",
    "        \n",
    "        # append items to the order_list\n",
    "        order_list.append(part)\n",
    "    \n",
    "    # return final order list\n",
    "    return order_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function clean_input to all rows to rows \n",
    "# this returns an array of items and their quantity ordered which is saved in items_clean_series\n",
    "items_clean_series = correct_rows.apply(lambda row: clean_input(row['shopping_cart']) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index values of the saved array of shopping items\n",
    "correct_rows.reset_index(drop = True, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dataframe to append our shopping items and their quantity to\n",
    "items_matrix = pd.DataFrame(columns=list(unique_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we seek to append all the shopping cart items to our matrix dataframe\n",
    "\n",
    "# create a counter and set the counter to 0 \n",
    "n = 0\n",
    "\n",
    "# iterate through array of shopping items\n",
    "for i in items_clean_series:\n",
    "    \n",
    "    # iterate through each tuple element\n",
    "    for tup in i:\n",
    "        \n",
    "        # locate the column name matching the shopping cart item and insert the quantity order\n",
    "        items_matrix.loc[n,tup[0]] = tup[1]\n",
    "    \n",
    "    # increase counter\n",
    "    n+=1\n",
    "\n",
    "# replace all Na values with 0\n",
    "items_matrix.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the orde_price for all of these cobinations of items and quantities\n",
    "items_matrix['price'] = correct_rows['order_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually check our result\n",
    "items_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to create a set of  multilinear equations to calculate individual item prices\n",
    "linear_equation_set = items_matrix.head(10)\n",
    "\n",
    "# save the names of items (10 items) as a list\n",
    "col_names_matrix_a = linear_equation_set.columns.to_list()\n",
    "\n",
    "# remove the last column name, price\n",
    "col_names_matrix_a.pop()\n",
    "\n",
    "# save these items and their names in a dataframe as part_a\n",
    "part_a = linear_equation_set[col_names_matrix_a]\n",
    "\n",
    "# save the price of these items in a Series as part_b\n",
    "part_b = linear_equation_set['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the to_numpy function to convert a dataframe to an array format\n",
    "a_input = part_a.to_numpy(dtype ='int')\n",
    "\n",
    "# use the to_numpy function to convert a dataframe to an array format\n",
    "b_input = part_b.to_numpy(dtype ='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the np.linalg.solve() function from the numpy library we are able to calculat the\n",
    "# price of each item using multilinear equations\n",
    "price_of_item_array = np.linalg.solve(a_input,b_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the linear_equation_set.columns and the price_of_item_array into one dictionary\n",
    "price_of_item = dict(zip(linear_equation_set.columns,price_of_item_array))\n",
    "\n",
    "# iterate through keys and value of the dictionary price_of_item\n",
    "for key,val in price_of_item.items():\n",
    "    \n",
    "    # save the rounded item value for as a key\n",
    "    price_of_item[key] = int(round(val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the order price of each shopping cart row value\n",
    "def calc_order_price(items):\n",
    "    \n",
    "    # save the important values which is product name and quantity\n",
    "    items_order = clean_input(items)\n",
    "    \n",
    "    # create an int variable for price \n",
    "    price = 0\n",
    "    \n",
    "    # itterate through the ordered items\n",
    "    for i in items_order:\n",
    "        \n",
    "        # calculate the combined price of all the items in the shopping cart \n",
    "        # considering both the item price and quantity\n",
    "        price = int(price + price_of_item[i[0]] * i[1])\n",
    "        \n",
    "    # return the overall price\n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that checks if calculated and given order price match\n",
    "def check_order_price(given,cal):\n",
    "    \n",
    "    # cheking if calculated and given order price are identical\n",
    "    if given - cal == 0:\n",
    "        return True\n",
    "    \n",
    "    # in all other cases return no\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that fixes the given order price\n",
    "def fix_order_price(given,cal):\n",
    "    \n",
    "    # in case of the given price not deviating from the calculated price\n",
    "    if given - cal == 0:\n",
    "        \n",
    "        # return the original price\n",
    "        return given\n",
    "    \n",
    "    # in all other cases\n",
    "    else:\n",
    "        \n",
    "        # return the calculated price\n",
    "        return cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the calc_order_price to all rows and save the result in a new column named 'correct_order_price'\n",
    "dirty_data_df['correct_order_price'] = dirty_data_df.apply(lambda row: calc_order_price(row['shopping_cart']) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the check_order_price to all rows \n",
    "# in cae a price deviates from the given price save it\n",
    "incorrect_price_series = dirty_data_df.apply(lambda row: check_order_price(row['order_price'],row['correct_order_price']) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for many incorrect order_price there are\n",
    "incorrect_price_series.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the index value of all the rows with faulty season values given the date to\n",
    "# the list fixed_columns\n",
    "[fixed_columns.append(x) for x in list(dirty_data_df.index[incorrect_price_series == False])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the function fix_order_price we calculate the correct order price for all rows and save it in the\n",
    "# orginal 'order_price' column \n",
    "dirty_data_df['order_price'] = dirty_data_df.apply(lambda row: fix_order_price(row['order_price'],row['correct_order_price']) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do final check if all order prices are correct \n",
    "incorrect_price_series = dirty_data_df.apply(lambda row: check_order_price(row['order_price'],row['correct_order_price']) ,axis=1)\n",
    "\n",
    "# display count\n",
    "incorrect_price_series.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.6 Check the order_total price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that calculates the correct order_total value \n",
    "def correct_order_total(order_price, discount, delivery):\n",
    "    \n",
    "    # calculate the discount for each order\n",
    "    discount = (100 - discount) * 0.01\n",
    "    \n",
    "    # calucalte the overall correct order_total \n",
    "    correct_total = round((order_price * discount) + delivery,2)\n",
    "    \n",
    "    # return new order total\n",
    "    return correct_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that checks if the order total is correct\n",
    "def check_order_total(order_total,correct_order_total):\n",
    "    \n",
    "    # check if calculated and given order_total are identical\n",
    "    if order_total - correct_order_total == 0:\n",
    "        \n",
    "        # if yes return True value\n",
    "        return True\n",
    "    \n",
    "    # in all other cases return False value\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the correct_order_total function calculate the correct_order_total and save it\n",
    "# in a new column named 'correct_order_total'\n",
    "dirty_data_df['correct_order_total'] = dirty_data_df.apply(lambda row: correct_order_total(row['order_price'],row['coupon_discount'],row['delivery_charges']) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct final check if all 'order_price' values are now correct\n",
    "correct_order_total_series = dirty_data_df.apply(lambda row: check_order_total(row['order_price'],row['correct_order_price']) ,axis=1)\n",
    "correct_order_total_series.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the index value of all rows wwith incorrect 'order_price's\n",
    "[fixed_columns.append(x) for x in list(dirty_data_df.index[correct_order_total_series == False])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\"> C. Drop unwanted columns and write fully clean dataframe to csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all the columns\n",
    "# to see which ones were added for calculation purposes and thus need to be dropped\n",
    "dirty_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we drop all the unwanted columns\n",
    "dirty_data_df.drop(['sentiment_score','cor_distance_to_nearest_warehouse', 'correct_order_price', 'correct_order_total'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final check to see in all unwanted columns were droped \n",
    "dirty_data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally we write the cleaned dataframe to a csv file \n",
    "dirty_data_df.to_csv('26255367_dirty_data_solution.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> 2. Removing outliers  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataframe we will locate and remove all outliers in the column delivery_charges.\n",
    "\n",
    "The term outlier referes to data points that vary greatly from the preceived norm. Some outliers might be 'justified' since they represnent certain contions, for example a very long delivery distance in the case of our delivery_charges. Thus, to access which outliers are justified and which aren't we created a Linear Regression Model for each season. The steps to identify ouliers and fix them are outlined below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Read in data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in csv file with outlier data\n",
    "outlier_data_df = pd.read_csv('26255367_outlier_data.csv')\n",
    "\n",
    "outlier_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Perpare dataframe for outlier analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that converts a boolean scale from True/Flase to 0/1\n",
    "def convert_boolean(series): \n",
    "    \n",
    "    # if value = True we repalce it with 1\n",
    "    if True: \n",
    "        \n",
    "        # return value 1\n",
    "        return 1 \n",
    "    \n",
    "    # if value = Flase we repalce it with 0\n",
    "    else: \n",
    "        \n",
    "        # return value 0\n",
    "        return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the apply and above defined convert_boolean function we convert True or False \n",
    "# into 1 or to for quantitative calculation in linear regression \n",
    "outlier_data_df['is_happy_customer'] = outlier_data_df['is_happy_customer'].apply(lambda row: convert_boolean(row))\n",
    "outlier_data_df['is_expedited_delivery'] = outlier_data_df['is_expedited_delivery'].apply(lambda row: convert_boolean(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Plot delivery charges in boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a boxplot of distribution of delivery charges grouped by season \n",
    "# outliers are observed in each season \n",
    "delivery_charges_outlier_bp = outlier_data_df.boxplot(column = 'delivery_charges', by = 'season')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Create Linear Regression Model for delivery_charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Linear Regression Model to predict delivery charges for each season\n",
    "\n",
    "# different season name \n",
    "seasons = {'Spring', 'Summer', 'Autumn', 'Winter'}\n",
    "\n",
    "# x variable for the multiple regression model \n",
    "# y variable is the target value we predict using the regression model \n",
    "X_var = ['is_happy_customer','is_expedited_delivery','distance_to_nearest_warehouse']\n",
    "Y_var = ['delivery_charges']\n",
    "\n",
    "# create list to save each season in\n",
    "seperated_df_by_season = []\n",
    "\n",
    "# loop through the season name \n",
    "for season in seasons: \n",
    "\n",
    "    # filter out the rows whose season name is the same as the loop \n",
    "    season = outlier_data_df[outlier_data_df.season == season]\n",
    "    \n",
    "    # instanciate linear regression function and fit the x and y variable specified above\n",
    "    regression_season = LinearRegression().fit(season[X_var], season[Y_var])\n",
    "    \n",
    "    # calculate the predicted delivery_charges based on the model built \n",
    "    # append the values to 'normalised_delivery_charges' column in each season \n",
    "    season['normalised_delivery_charges']= regression_season.predict(season[X_var])\n",
    "    \n",
    "    # the difference between the true value and the predicted value from the model above \n",
    "    season['residuals'] = season['delivery_charges'] - season['normalised_delivery_charges']\n",
    "    \n",
    "    \n",
    "    # after calculating the residuals and predicted values with the linear regression model, \n",
    "    # append the data frame of each season into a list for concatenation at the end\n",
    "    seperated_df_by_season.append(season)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Plot the  delivery_charges to get overview of data spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the calculated delivery charges for each season using histograms \n",
    "\n",
    "# plot spring charges \n",
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(1, 1, 1)\n",
    "n, bins, patches = ax1.hist(seperated_df_by_season[0]['delivery_charges'], color='r')\n",
    "ax1.set_xlabel('Delivery Charges')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Spring')\n",
    "\n",
    "# plot summer charges \n",
    "fig2 = plt.figure()\n",
    "ax2 = fig2.add_subplot(1, 1, 1)\n",
    "n, bins, patches = ax2.hist(seperated_df_by_season[1]['delivery_charges'])\n",
    "ax2.set_xlabel('Delivery Charges')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Summer')\n",
    "\n",
    "# plot autumn charges \n",
    "fig3 = plt.figure()\n",
    "ax3 = fig3.add_subplot(1, 1, 1)\n",
    "n, bins, patches = ax3.hist(seperated_df_by_season[2]['delivery_charges'], color='black')\n",
    "ax3.set_xlabel('Delivery Charges')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('Autumn')\n",
    "\n",
    "# plot winter charges \n",
    "fig4 = plt.figure()\n",
    "ax4 = fig4.add_subplot(1, 1, 1)\n",
    "n, bins, patches = ax4.hist(seperated_df_by_season[3]['delivery_charges'], color='green')\n",
    "ax4.set_xlabel('Delivery Charges')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.set_title('Winter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the delivery charges of each season using bloxplots. This helps use to easily visualize outliers (above and blow the upper and lower whisker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the boxplot for spring delivery charges\n",
    "ax_spring = seperated_df_by_season[0].boxplot('residuals', figsize = (10,6))\n",
    "ax_spring.set_title('Residual Plot: Spring')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the boxplot for summer delivery charges\n",
    "ax_summer = seperated_df_by_season[1].boxplot('residuals', figsize = (10,6))\n",
    "ax_summer.set_title('Residual Plot: Summer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the boxplot for autumn delivery charges\n",
    "ax_autumn = seperated_df_by_season[2].boxplot('residuals', figsize = (10,6))\n",
    "ax_autumn.set_title('Residual Plot: Autumn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the boxplot for winter delivery charges\n",
    "ax_winter = seperated_df_by_season[3].boxplot('residuals', figsize = (10,6))\n",
    "ax_winter.set_title('Residual Plot: Winter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Compute upper and lower whiskers to determine outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After plotting the various delivery charges we can see that there outliers in each season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that calculates iqr and the whiskers given the q1 and q3 values as input\n",
    "def compute_whiskers(Q1, Q3):\n",
    "    \n",
    "    # get iqr\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # get lower whisker \n",
    "    lower_w = Q1 - 1.5 * IQR\n",
    "    \n",
    "    # get upper whisker \n",
    "    upper_w = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # return the iqr and both whiskers\n",
    "    return IQR, lower_w, upper_w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Determine outliers and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the outliers and remove them \n",
    "\n",
    "# looping through each data frame in seperated_df_by_season list\n",
    "for season in seperated_df_by_season: \n",
    "    \n",
    "    # the first quartile, 1/4th of the data points \n",
    "    Q1_season = season['residuals'].quantile(0.25)\n",
    "    \n",
    "    # the third quartile, 3/4 of the data points \n",
    "    Q3_season = season['residuals'].quantile(0.75)\n",
    "    \n",
    "    # IQR, lower whisker, and upper whisker returned by compute_whiskers function\n",
    "    IQR_season, lower_season, upper_season = compute_whiskers(Q1_season, Q3_season)\n",
    "    \n",
    "    # if the value is less than the lower whisker or higher than the upper whisker, \n",
    "    # drop the rows\n",
    "    season.drop(season[(season.residuals < lower_season) |(season.residuals > upper_season)].index,\\\n",
    "                inplace=True)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat() concatenate all the data frame in seperated_df_by_season list \n",
    "# creating boxplot of the delivery_charges grouped by each season \n",
    "# outliers are still detected after the first round of outlier removal\n",
    "# repeating the same procedure to remove another round of outliers \n",
    "pd.concat(seperated_df_by_season).boxplot(column = 'delivery_charges', by = 'season')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the outliers again and remove the remaining ones \n",
    "\n",
    "seperated_df_by_season_follow_up =[]\n",
    "\n",
    "# looping through each data frame in seperated_df_by_season list\n",
    "for season in seperated_df_by_season:\n",
    "    \n",
    "    regression_season = LinearRegression().fit(season[X_var], season[Y_var])\n",
    "    \n",
    "    season['normalised_delivery_charges']= regression_season.predict(season[X_var])\n",
    "    \n",
    "    season['residuals'] = season['delivery_charges'] - season['normalised_delivery_charges']\n",
    "    \n",
    "    Q1_season = season['residuals'].quantile(0.25)\n",
    "    \n",
    "    Q3_season = season['residuals'].quantile(0.75)\n",
    "    \n",
    "    IQR_season, lower_season, upper_season = compute_whiskers(Q1_season, Q3_season)\n",
    "    \n",
    "    \n",
    "    # if the value is less than the lower whisker or higher than the upper whisker, drop the rows\n",
    "    season.drop(season[(season.residuals < lower_season) |(season.residuals > upper_season)].index, inplace=True)\n",
    "    \n",
    "    seperated_df_by_season_follow_up.append(season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# do final check to see if there are no more outliers in delivery_charges detected \n",
    "pd.concat(seperated_df_by_season_follow_up).boxplot(column = 'delivery_charges', by = 'season')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit was successfull and we could not detect more outliers in delivery_charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the dataframes in list seperated_df_by_season_follow_up\n",
    "outlier_removal_solution = pd.concat(seperated_df_by_season_follow_up)\n",
    "\n",
    "# sorting based on original index\n",
    "outlier_removal_solution.sort_index(inplace = True)\n",
    "\n",
    "# display how many outliers were detected and removed alltogether \n",
    "# in the column delivery_charges\n",
    "outlier_data_df.shape[0]-outlier_removal_solution.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Drop unwanted columns and write clean dataframe to csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the columns in outlier df and identify unwanted ones\n",
    "outlier_removal_solution.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that 'normalised_delivery_charges','residuals' are no longer needed and thus\n",
    "# can be dropped\n",
    "# now drop all the unwanted columns\n",
    "outlier_removal_solution.drop(['normalised_delivery_charges','residuals'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_removal_solution.to_csv('26255367_outlier_data_solution.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\"> 3. Imputing missing values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataframe we will locatye and fix all 'coverage' errors. To impute all the missing values we need to a) find the columns affected by missing values and b) find methods to impute such missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Read in data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in our missing dataf file in csv format\n",
    "missing_data_df = pd.read_csv('26255367_missing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display first 10 rows\n",
    "missing_data_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently missing values have NaN value inserted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Find all columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows dimensions of the dataframe\n",
    "missing_data_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows information of all columns\n",
    "missing_data_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this check we know that there are missing values (NaN values) in 6 columns, which are:\n",
    "\n",
    "- nearest_warehouse\n",
    "- order_price\n",
    "- delivery_charges\n",
    "- order_total\n",
    "- distance_to_nearest_warehouse\n",
    "- is_happy_customer\n",
    "\n",
    "For all of these columns we will now impute the missing values based on the surrounding information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Impute missing is_happy_customer values\n",
    "\n",
    "Values in the is_happy_customer column can only take a boolean value (True = 1, False = 0). We will determine the is_happy_customer based on the review the customer left. Thus, we can reuse the function created in the first part of the assignment where we edited the dirty_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function get_sentiment to the column latest_customer_review to\n",
    "# calculate a sentiment score\n",
    "# that sentiment score is then saved in a new column called \n",
    "# missing_data_df['sentiment_score']\n",
    "missing_data_df['sentiment_score'] = missing_data_df.apply(lambda row: get_sentiment(row['latest_customer_review'])[\"compound\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fix the is_happy_customer boolean values \n",
    "def fix_happy_missing_val(score):\n",
    "    \n",
    "    # return happy True if sentiment_score >= 0.05\n",
    "    if score  >= 0.05: \n",
    "        return 1.0\n",
    "    \n",
    "    # return happy False in all other case\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we imput all the missing is_happy_customer values values\n",
    "# we fill them by applying the fix_happy_missing_val methode to missing_data_df['is_happy_customer'] column\n",
    "missing_data_df['is_happy_customer'] = missing_data_df.apply(lambda row: fix_happy_missing_val(row['sentiment_score']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct a final check if all the missing values were calculate\n",
    "missing_data_df['is_happy_customer'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final check showed that there are now 500 values in the missing_data_df['is_happy_customer'] column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Impute missing nearest_warehouse values\n",
    "\n",
    "To impute the missing values for nearest_warehouse we will use the read in coordinate information of of the warehouses and compare them with the customer's coordinates. The we will calculate the distance to each warehouse and choose the shortest distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the names of the three warehouse \n",
    "warehouse_df = pd.read_csv('warehouses.csv')\n",
    "\n",
    "warehouse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate which warehouse is the nearest warehouse to the customer\n",
    "# done using the compare_dist_warehouse() function that was defined in part 1 of the assignment\n",
    "missing_data_df['nearest_warehouse'] = missing_data_df.apply(lambda row: compare_dist_warehouse(row['customer_lat'],row['customer_long']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct a final check if all the missing values were calculate\n",
    "missing_data_df['nearest_warehouse'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final check showed that there are now 500 values in the missing_data_df['nearest_warehouse'] column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Impute missing distance_to_nearest_warehouse values\n",
    "\n",
    "To impute the missing values for distance_to_nearest_warehouse we will use the read in coordinate information of of the warehouses and customer's coordinates. Then we will calculate the distance to the warehouse using the cal_warehouse_dist function defined in the first part of the assignmnet ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the correct distance from a customer to the nearest warehouse and save that value\n",
    "# column missing_data_df['distance_to_nearest_warehouse']\n",
    "missing_data_df['distance_to_nearest_warehouse'] = missing_data_df.apply(lambda row: cal_warehouse_dist(row['nearest_warehouse'],row['customer_lat'],row['customer_long']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct a final check if all the missing values were calculate\n",
    "missing_data_df['distance_to_nearest_warehouse'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final check showed that there are now 500 values in the missing_data_df['distance_to_nearest_warehouse'] column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Impute missing order_price values\n",
    "\n",
    "To impute the missing values for order_price we will use the calc_order_price() function defined in part one of \n",
    "the assignment. This function imputes missing values for order_price using the items and their quantity in the column 'shopping_cart.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the calc_order_price to all rows in column 'shopping_cart' and save the result\n",
    "# in missing_data_df['order_price']\n",
    "missing_data_df['order_price'] = missing_data_df.apply(lambda row: calc_order_price(row['shopping_cart']) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct a final check if all the missing values were calculate\n",
    "missing_data_df['order_price'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final check showed that there are now 500 values in the missing_data_df['order_price'] column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Impute missing order_total values\n",
    "\n",
    "To impute the missing values for order_total we can use the realtionship that exists between the columns:\n",
    "\n",
    "- order_price\n",
    "- delivery_charges\n",
    "- order_total\n",
    "\n",
    "The column order_price represents the combined price of all ordere items PRIOR to applying any discounts or adding\n",
    "delivery fees. Since order_total values are derived by adding together the order_price with applied coupon_discount and the delivery_charges we can calculate the order_total values using the order_price, coupon_discount, and delivery_charges. This realtionship between the four variables can be expresed with the following formula:\n",
    "\n",
    "order_price * ((100 - coupon_discount) / 100) + delivery_charges = order_total\n",
    "\n",
    "In order for this formular to work we need to check that in the case of order_total values missing the \n",
    "other values are present. We know that the values for order_price and coupon_discount are complete. Thus, we only need to check whether in the rows where order_total values are missing the delivery_charges are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all required values for order_total value calculation are present\n",
    "# checking overlap using isnull() isin() and .index[] functions\n",
    "missing_data_df.index[missing_data_df.delivery_charges.isnull()].isin(set(missing_data_df.index[missing_data_df.order_total.isnull()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We confirmed that in the rows where order_total values are missing the delivery_charges are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that calculates the total_oder price based on the relationship between\n",
    "# order_price, delivery_charges, and order_total\n",
    "\n",
    "def calc_order_total(order_price, discount, delivery_charges):\n",
    "    \n",
    "    # apply the above defined formula\n",
    "    total_order_price = round(order_price * ((100-discount)/100) + delivery_charges,2)\n",
    "    \n",
    "    # return the calculated total_order value\n",
    "    return total_order_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all the rows where the order_total value is missing\n",
    "# and save it as a list \n",
    "total_order_miss_row = missing_data_df[missing_data_df.order_total.isnull()].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the calc_order_total to all rows in column 'order_total' and save the result\n",
    "# in missing_data_df['order_price']\n",
    "missing_data_df.iloc[total_order_miss_row,missing_data_df.columns.get_loc('order_total')] = missing_data_df.iloc[total_order_miss_row].apply(lambda row: calc_order_total(row['order_price'],row['coupon_discount'],row['delivery_charges']) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any missing values for order_total remain\n",
    "missing_data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final check showed that there are now 500 values in the missing_data_df['order_total'] column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Impute missing delivery_charges values\n",
    "\n",
    "To impute the missing values for delivery_charges we can use the realtionship that exists between the columns:\n",
    "\n",
    "- order_price\n",
    "- delivery_charges\n",
    "- order_total\n",
    "\n",
    "The column order_price represents the combined price of all ordere items PRIOR to applying any discounts or adding\n",
    "delivery fees. Since order_total values are derived by adding together the order_price with applied coupon_discount and the delivery_charges we can calculate the order_total values using the order_price, coupon_discount, and delivery_charges. This realtionship between the four variables can be expresed with the following formula:\n",
    "\n",
    "order_total - order_price * ((100 - coupon_discount) / 100) = delivery_charges\n",
    "\n",
    "In order for this formular to work we need to check that in the case of order_total values missing the \n",
    "other values are present. We know that the values for order_price and coupon_discount are complete. Thus, we only need to check whether in the rows where order_total values are missing the delivery_charges are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all required values for  delivery_charges value calculation are present\n",
    "# checking overlap using isnull() isin() and .index[] functions\n",
    "missing_data_df.index[missing_data_df.delivery_charges.isnull()].isin(set(missing_data_df.index[missing_data_df.order_total.isnull()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We confirmed that in the rows where order_total values are missing the delivery_charges are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all the rows where the delivery_charges value is missing\n",
    "# and save it as a list \n",
    "delivery_charges_miss_row = missing_data_df[missing_data_df.delivery_charges.isnull()].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delivery_charges_miss_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that calculates the delivery charges based on\n",
    "# order_price, discount, and order_total\n",
    "def calc_delivery_charges(total_order_price, order_price, discount):\n",
    "    \n",
    "    # apply the above defined formula\n",
    "    delivery_charges = round(total_order_price - order_price * ((100-discount)/100), 2)\n",
    "\n",
    "    # return the calculated delivery_charges \n",
    "    return delivery_charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the calc_order_total to all rows in column 'order_total' and save the result\n",
    "# in missing_data_df['order_price']\n",
    "missing_data_df.iloc[delivery_charges_miss_row,missing_data_df.columns.get_loc('delivery_charges')] = missing_data_df.iloc[delivery_charges_miss_row].apply(lambda row: calc_delivery_charges(row['order_total'],row['order_price'],row['coupon_discount']) ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any missing values for 'order_total' remain\n",
    "missing_data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final check showed that there are now 500 values in the missing_data_df['delivery_chargesl'] column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10 Drop unwanted columns and write final dataframe to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for unwanted columns \n",
    "missing_data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we drop all the unwanted columns\n",
    "missing_data_df.drop('sentiment_score', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we successfully dropped all unwanted columns \n",
    "missing_data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally we write the edited dataframe to a csv file \n",
    "missing_data_df.to_csv('26255367_missing_data_solution.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, I have been able to complete all three tasks:\n",
    "\n",
    "- 'clean' inaccurate data\n",
    "- remove outliers\n",
    "- impute missing values \n",
    "\n",
    "Through out the assignment I had a couple of major take-ways.\n",
    "Firstly, the combination of apply(), and lambda functions in combination with custome built functions proved to be the most efficient way to edit pandas dataframes. Secondly, an overarching technique I found to be very usefull, and that I applied to all of the wrangling tasks , was to do a 'check-up' after each step, to see if the desired changes were succefully implemented and I can move to the next step. Thirdly, one of the main benefits of finishing the cleaning dirty data task first, was that we could re-use functions and techniques applied in the first part for the thrid part where we imputed missing values. \n",
    "\n",
    "Overall the assignment was a great and fun exercise to get familiar with data wrangling techniques and pandas dataframe manipulations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. References\n",
    "\n",
    "- Since the tutorial material was very comprehensive I didn't need a lot of out-of-class materials, outside of the resources given in the assignment brief, general pandas, and python documentation.\n",
    "\n",
    "https://docs.python.org/3/\n",
    "\n",
    "https://pandas.pydata.org/docs/\n",
    "\n",
    "https://community.esri.com/groups/coordinate-reference-systems/blog/2017/10/05/haversine-formula\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/linalg.html#module-scipy.linalg\n",
    "\n",
    "https://riptutorial.com/numpy/example/12987/solve-linear-systems-with-np-solve\n",
    "\n",
    "https://www.kdnuggets.com/2019/06/select-rows-columns-pandas.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
